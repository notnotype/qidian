import logging
import click

from urllib.parse import urljoin
from requests import get
from lxml.etree import HTML
from json import dumps, loads
from FontIconMappingTable import FontIconMappingTable, translate

from time import localtime

_get = get
logger = logging.Logger("main")
handler = logging.FileHandler(f"logs/"
                              f"{localtime().tm_year}-"
                              f"{localtime().tm_mon}-"
                              f"{localtime().tm_mday}--"
                              f"{localtime().tm_hour}h-"
                              f"{localtime().tm_min}m-"
                              f"{localtime().tm_sec}s.log",
                              encoding="utf-8")
formatter = logging.Formatter("[%(levelname)-5.5s][%(funcName)-7.7s][%(lineno)3.3dè¡Œ]-%(message)s")
handler.setFormatter(formatter)
logger.addHandler(handler)

console = logging.StreamHandler()
console.setLevel(logging.INFO)
console.setFormatter(formatter)

logger.addHandler(handler)
logger.addHandler(console)

TIMEOUT = 10
HEADERS = {"User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chr"
                         "ome/86.0.4240.111 Safari/537.36"}
OUTFILE = f"out/{localtime().tm_year}-{localtime().tm_mon}-{localtime().tm_mday}--{localtime().tm_hour}h-{localtime().tm_min}m-{localtime().tm_sec}s.out.json"


# åŒ…è£…getæ–¹æ³•
def get(*args, **kwargs):
    retry = 0
    while retry <= 3:
        try:
            if TIMEOUT:
                kwargs["timeout"] = TIMEOUT
            if "headers" not in kwargs:
                kwargs["headers"] = get_headers()
            logger.debug("[è·å–ç½‘é¡µ] {}".format(args[0]))
            logger.debug(f"[ä½¿ç”¨] headers [{kwargs['headers']}]  -  timeout [{kwargs['timeout']}]")
            return _get(*args, **kwargs)
        except Exception as e:
            logger.error("[è·å–ç½‘é¡µå¤±è´¥]: ")
            logger.exception(e)
            retry += 1
    return None


def get_headers():
    return HEADERS


def get_api(chan_id, sub_cata_id, page):
    url = f"https://www.qidian.com/all?chanId={chan_id}&subcataId={sub_cata_id}&orderId=&page={page}&style=" \
          f"2&pageSize=50&siteid=1&pubflag=0&hiddenField=0"
    return url


class OutFile:
    def __init__(self, filename: str):
        self.f = open(filename, "a+", encoding="utf8")

    def save(self, item: dict):
        self.f.write(dumps(item).encode().decode("unicode_escape") + "\n")
        self.f.flush()

    def close(self):
        self.f.close()


# noinspection PyBroadException
def get_detailed_info(novel_url) -> dict:
    novel_id = novel_url[novel_url.rindex("/") + 1:]
    resp = get(novel_url, headers=get_headers())
    html = HTML(resp.text)

    book_info = html.xpath("//div[@class='book-info ']")[0]

    item = {}

    # æ ‡ç­¾
    tag = [e.xpath("./text()")[0] for e in book_info[1]]

    # æè¿°
    intro = book_info[2].xpath("./text()")[0]

    # ç ´è§£å­—ä½“å›¾æ ‡åŠ å¯†
    fimt = FontIconMappingTable(html, get_headers())
    mapping_table = fimt.get_table()

    # æ¨è
    num = [translate(i, mapping_table) for i in book_info[3].xpath(".//em/span/text()")]
    unit = book_info[3].xpath(".//cite/text()")
    recommend = list(zip(num, unit))

    # è·å–è¯„è®º
    resp = get(f"https://book.qidian.com/ajax/comment/index?bookId={novel_id}&pageSize=15", headers=get_headers())
    resp_json = resp.json()

    rate = translate(resp_json["data"]["rate"], mapping_table)
    user_counts = translate(resp_json["data"]["userCount"], mapping_table)
    # è¿™ä¸ªåœ°æ–¹å–œæ¬¢æŠ¥é”™, ç°åœ¨ä¸æŠ¥é”™äº†,å˜¿å˜¿
    try:
        # å…ˆç”¨è€api, åœ¨å°è¯•æ–°api
        # è€api, å–œæ¬¢æŠ¥é”™
        logger.debug("ä½¿ç”¨è€api")
        chapter_counts = translate(html.xpath("//span[@id='J-catalogCount']/text()")[0], mapping_table)
    except Exception as e:
        # ä½¿ç”¨æ–°api
        logger.debug("è€apiå‡ºé”™ ä½¿ç”¨æ–°api åŸå› :", exc_info=True)
        chapter_counts = translate(get(f"https://book.qidian.com/ajax/book/category?bookId={novel_id}",
                                       headers=get_headers()).json()["data"]["chapterTotalCnt"], mapping_table)

    # æŠŠæ•°æ®æ·»åŠ åˆ°item
    item["tag"] = tag
    item["intro"] = intro
    item["recommend"] = recommend
    item["rate"] = rate
    item["user_counts"] = user_counts
    item["chapter_counts"] = chapter_counts
    return item


# noinspection PyBroadException
def _spider(chan_id, sub_cata_id, page):
    global outfile
    outfile = OutFile(OUTFILE)
    # ä¿¡ä»»getæ–¹æ³•ä¸ä¼šæŠ¥é”™
    resp = get(get_api(chan_id, sub_cata_id, page), headers=get_headers())

    # å¦‚æœ `æ²¡æœ‰æ‰¾åˆ°ç¬¦åˆæ¡ä»¶çš„ä¹¦` åœ¨ç½‘é¡µé‡Œé¢, åˆ™è¿”å›
    if not resp or "æ²¡æœ‰æ‰¾åˆ°ç¬¦åˆæ¡ä»¶çš„ä¹¦" in resp.text:
        logger.info(f"çˆ¬è¡Œåˆ°åº•äº† [{chan_id}]")
        return None

    html = HTML(resp.text)
    novels = []

    # è§£ææ–‡æ¡£
    tbody = html.xpath("//tbody")[0]
    for tr in tbody:
        # æ¯ä¸€ä¸ªé¡¹ç›®çš„ä¿¡æ¯
        item = {}

        # ç±»åˆ«
        cata = (tr[0][0].xpath("./text()")[0], tr[0][-1].xpath("./text()")[0])
        # ä¹¦å
        name = tr[1][0].xpath("./text()")[0]
        # é“¾æ¥
        novel_url = urljoin("https://www.qidian.com/", tr[1][0].xpath(".//@href")[0])
        # è§£æid
        novel_id = novel_url[novel_url.rindex("/") + 1:]
        # è¿›å…¥ç®€ä»‹é¡µé¢è·å–ä¿¡æ¯

        try:
            detailed_info = get_detailed_info(novel_url).items()
        except Exception as e:
            logger.error(f"[æ•°æ®æŠ“å–é”™è¯¯] {name}")
            logger.exception(e)
            continue

        for key, value in detailed_info:
            item[key] = value

        # æœ€åä¸€ç« 
        last_chapter = tr[2][0].xpath("./text()")[0]
        # å­—æ•°
        # done æ·»åŠ å•ä½
        '''
            <span class="total" xpath="1">
                <style>@font-face { font-family: sCAWJsOr; src: url('https://qidian.gtimg.com/qd_anti_spider/sCAWJsOr.eot?') format('eot'); src: url('https://qidian.gtimg.com/qd_anti_spider/sCAWJsOr.woff') format('woff'), url('https://qidian.gtimg.com/qd_anti_spider/sCAWJsOr.ttf') format('truetype'); } .sCAWJsOr { font-family: 'sCAWJsOr' !important;     display: initial !important; color: inherit !important; vertical-align: initial !important; }</style>
                <span class="sCAWJsOr">ğ˜œ’ğ˜œ”ğ˜œ™ğ˜œ•ğ˜œ™ğ˜œ™</span>ä¸‡
            </span>
        '''
        # word_counts = tr[3][0][-1].xpath("./text()")[0]
        # ä½œè€…
        author = tr[4][0].xpath("./text()")[0]
        update_date = tr[5].xpath("./text()")[0]

        # å°è£…æ•°æ®
        item["cata"] = cata
        item["name"] = name
        item["last_chapter"] = last_chapter
        # item["word_count"] = word_counts
        item["author"] = author
        item["update_date"] = update_date
        item["novel_id"] = novel_id

        novels.append(item)
        outfile.save(item)

        logger.info("[å¯¼å‡ºæ•°æ®] {}-{}-{}-{} ...".format(
            item["novel_id"],
            item["name"],
            item["author"],
            item["update_date"]
        ))
    return novels


# ä¸»é€»è¾‘
def _main(chan_id, sub_cata_id, headers, timeout, outfile):
    global TIMEOUT
    global HEADERS
    global OUTFILE
    TIMEOUT = timeout
    if outfile:
        OUTFILE = outfile
    for i in range(1, 3):
        _spider(chan_id, sub_cata_id, str(i))


# åŒ…è£…click
# clickç»„
@click.group()
def main():
    """å°è¯•è¾“å…¥`python main.py spider --help`æ¥è·å–å¸®åŠ©"""
    pass


# click spiderå‘½ä»¤
@click.command()
@click.option("--chan-id", "-ci", help="å¤§ç±»id")
@click.option("--sub-cata-id", "-sci", help="å°ç±»id")
@click.option("--headers", "-h", help="æºå¸¦è¯·æ±‚å¤´æ–‡ä»¶", type=click.File())
@click.option("--timeout", "-t", default=15.0, help="è®¾ç½®è¯·æ±‚è¶…æ—¶æ—¶é—´")
@click.option("--outfile", "-o", help="è®¾ç½®è¾“å‡ºæ–‡ä»¶")
@click.option("--fromfile", "-f", help="ä»æ–‡ä»¶åŠ è½½æ•°æ®ç»§ç»­çˆ¬å–", type=click.File())
@click.option("--debug", "-d", help="å¯ç”¨è°ƒè¯•", type=click.BOOL, default=False)
def spider(chan_id, sub_cata_id, headers, timeout, outfile, fromfile, debug):
    """çˆ¬å–å¤§ç±»chan_id, å°ç±»sub_cata_idä¸‹çš„æ‰€æœ‰æ•°æ®   å°è¯•è¾“å…¥`python main.py spider --help`æ¥è·å–å¸®åŠ©"""
    if not sub_cata_id or not chan_id:
        if fromfile:
            pass
        else:
            ctx_help = click.get_current_context().get_help()
            click.echo("ç¼ºå¤±å‚æ•°: å¤§ç±»id, å°ç±»id\n")
            click.echo("å°è¯•è¾“å…¥`python spider -ci 1 -sci 1`æ¥è¿è¡Œçˆ¬è™«\n")
            click.echo(ctx_help)
            exit(0)
    global TIMEOUT
    global HEADERS
    global OUTFILE
    logger.debug("debug: " + str(debug))
    if debug:
        logger.setLevel(logging.DEBUG)
        console.setLevel(logging.DEBUG)
        handler.setLevel(logging.DEBUG)
        logger.addHandler(handler)
        logger.addHandler(console)
        logger.debug("[è°ƒè¯•æ¨¡å¼] å¯åŠ¨")
    logger.debug(f"[æ¥å—å‚æ•°]: "
                 f"[chan_id]: [{chan_id}], "
                 f"[sub_cata_id]: [{sub_cata_id}], "
                 f"[outfile]: [{outfile}], "
                 f"[fromfile]: [{fromfile}], "
                 f"[timeout]: [{timeout}], "
                 f"[headers]: [{headers}], "
                 f"[outfile]: [{outfile}], "
                 f"[debug]: [{debug}]"
                 )

    # å¦‚æœæœ‰å¤´æ–‡ä»¶åˆ™åŠ è½½å¤´æ–‡ä»¶
    if headers:
        global HEADERS
        HEADERS = loads(headers.read())

    TIMEOUT = timeout
    if outfile:
        OUTFILE = outfile
    # é€‰æ‹©æ˜¯å¦ä»æ–‡ä»¶çˆ¬å–
    if not fromfile:
        # ä¸ä»æ–‡ä»¶çˆ¬å–
        for i in range(1, 3):
            _spider(chan_id, sub_cata_id, str(i))
        return
    else:
        # ä»æ–‡ä»¶çˆ¬å–
        for line in fromfile:
            _chan_id, _sub_cata_id = line.split(" ")
            for i in range(1, 3):
                _spider(_chan_id.strip(), _sub_cata_id.strip(), str(i))


main.add_command(spider)
if __name__ == '__main__':
    main()

